model:
  n_layers: 12
  n_heads: 12
  d_model: 768
  d_ff: 3072
  vocab_size: 50257
  max_seq_len: 1024
  dropout: 0.1

train:
  batch_size: 32
  learning_rate: 3.0e-4
  weight_decay: 0.1
  max_steps: 100000
  max_tokens: 100000000
  warmup_steps: 40
  grad_clip: 1.0
  eval_interval: 100
  checkpoint_interval: 200
  grad_accum_steps: 8
  compile: true

data:
  dataset: openwebtext
  dataset_size: 100M
  tokenized_dir: datasets/tokenized/openwebtext_100M

metrics:
  wandb_project: nash-llm
  wandb_enabled: true
  log_interval: 10
  metrics:
    - val_loss
    - accuracy
