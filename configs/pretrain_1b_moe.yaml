model:
  n_layers: 12
  n_heads: 12
  d_model: 768
  d_ff: 3072
  vocab_size: 50257
  max_seq_len: 1024
  dropout: 0.1
  moe_enabled: true
  moe_num_experts: 8
  moe_top_k: 2
  moe_expert_d_ff: 1536
  moe_capacity_factor: 1.25
  moe_router_jitter: 0.01
  moe_start_layer: 6
  moe_layer_stride: 2

train:
  batch_size: 64
  learning_rate: 7.0e-4
  weight_decay: 0.1
  max_steps: 1000000
  max_tokens: 1000000000
  warmup_steps: 400
  grad_clip: 1.0
  eval_interval: 1000
  checkpoint_interval: 2000
  grad_accum_steps: 4
  compile: true
  precision: bf16
  muon_lr: 0.02
  muon_momentum: 0.95
  ns_steps: 5
  moe_aux_loss_coef: 0.01
  moe_z_loss_coef: 1.0e-4

data:
  dataset: fineweb
  dataset_size: 1B
  tokenized_dir: datasets/tokenized/fineweb_1B

metrics:
  wandb_project: nash-llm
  wandb_enabled: true
  log_interval: 20
  metrics:
    - val_loss
    - accuracy
